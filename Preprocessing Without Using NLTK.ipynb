{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first step is to combine text of each files\n",
    "combined_text = \"\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    file_name = \"corpus/\"+str(i)+\".txt\"\n",
    "    with open(file_name,'r') as file:\n",
    "        combined_text = combined_text + file.read()\n",
    "\n",
    "#second step is to apply tokenization \n",
    "tokenized_word = combined_text.split()\n",
    "\n",
    "tokenized_word_updated = [] #this list will contains tokens after removing penctuation marks and other symbols \n",
    "\n",
    "for i in tokenized_word:\n",
    "    temp = \"\"\n",
    "    for j in i:\n",
    "        if j.isalnum():\n",
    "            temp = temp + str(j)\n",
    "    if temp != \"\":\n",
    "        tokenized_word_updated.append(temp)\n",
    "\n",
    "#applying case conversion\n",
    "\n",
    "tokenized_word_after_case_conversion = [i.lower() for i in tokenized_word_updated]\n",
    "\n",
    "# applying stop word removal\n",
    "\n",
    "stopwords = ['is','the','of','that','are','to','an','from','a','those','can','be','on','or','other','for','in','and','what','has','been','into','does','not','with','by','as','so','how','each','then','if','this','when']\n",
    "\n",
    "after_stop_word_removal = []\n",
    "\n",
    "for i in tokenized_word_after_case_conversion:\n",
    "    if i not in stopwords:\n",
    "        after_stop_word_removal.append(i)\n",
    "        \n",
    "#applying stemming\n",
    "\n",
    "stem_word_map = {'information' : 'inform', 'retrieval' : 'retriev','activity' : 'activ','obtaining' : 'obtain','resources' : 'resourc','relevancyan' : 'relev','collection' : 'collect','searches' : 'search','based' : 'base','contentbased' : 'contentbas','indexing' : 'index','science' : 'scienc','searching' : 'search','documents' : 'document','themselves' : 'themselv','describes' : 'describ','databases' : 'databas','texts' : 'text','images' : 'imag','soundsautomated' : 'soundsautom','systems' : 'system','used' : 'use','reduce' : 'reduc','called' : 'call','software' : 'softwar','provides' : 'provid','books' : 'book','journals' : 'journal','stores' : 'store','manages' : 'manag','engines' : 'engin','visible' : 'visibl','applicationsan' : 'application','begins' : 'begin','enters' : 'enter','query' : 'queri','queries' : 'queri','statements' : 'statement','needs' : 'need','example' : 'exampl','strings' : 'string','uniquely' : 'uniqu','identify' : 'identifi','single' : 'singl','several' : 'sever','perhaps' : 'perhap','different' : 'differ','degrees' : 'degre','elevancy': 'relev','entity' : 'entiti','represented' : 'repres','database' : 'databas','matched' : 'match','however' : 'howev','opposed' : 'oppos','classical' : 'classic','results' : 'result','returned' : 'return','typically' : 'typic','ranked' : 'rank','ranking' : 'rank','difference' : 'differ','compared' : 'compar','compute' : 'comput','numeric' : 'numer','matches' : 'match','according' : 'accord','value' : 'valu','iterated' : 'iter','wishes' : 'wish','refine' : 'refin'}\n",
    "\n",
    "after_stemming = []\n",
    "\n",
    "for i in after_stop_word_removal:\n",
    "    if i in stem_word_map:\n",
    "        after_stemming.append(stem_word_map[i])\n",
    "    else:\n",
    "        after_stemming.append(i)\n",
    "\n",
    "after_stemming\n",
    "\n",
    "#final step\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for i in after_stemming:\n",
    "    if i not in vocab:\n",
    "        vocab.append(i)\n",
    "\n",
    "vocab.sort()\n",
    "\n",
    "#to represent each document in boolean model\n",
    "\n",
    "document_tokenized_text = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    file_name = \"corpus/\"+str(i)+\".txt\"\n",
    "    with open(file_name,'r') as file:\n",
    "        text = file.read()\n",
    "        text = text.split()\n",
    "        document_tokenized_text.append(text)\n",
    "\n",
    "document_tokenized_updated = []\n",
    "\n",
    "for i in document_tokenized_text:\n",
    "    lis = []\n",
    "    for j in i:\n",
    "        temp = \"\"\n",
    "        for k in j:\n",
    "            if k.isalnum():\n",
    "                temp = temp + k\n",
    "        if temp != \"\":\n",
    "            lis.append(temp)\n",
    "    document_tokenized_updated.append(lis)\n",
    "\n",
    "document_case_converted = []\n",
    "\n",
    "for i in document_tokenized_updated:\n",
    "    lis = []\n",
    "    for j in i:\n",
    "        lis.append(j.lower())\n",
    "    document_case_converted.append(lis)\n",
    "    \n",
    "document_after_stopword_removal = []\n",
    "\n",
    "for i in document_case_converted:\n",
    "    lis = []\n",
    "    for j in i:\n",
    "        if j not in stopwords:\n",
    "            lis.append(j)    \n",
    "    document_after_stopword_removal.append(lis)\n",
    "    \n",
    "document_after_stemming = []\n",
    "\n",
    "for i in document_after_stopword_removal:\n",
    "    lis = []\n",
    "    for j in i:\n",
    "        if j in stem_word_map:\n",
    "            lis.append(stem_word_map[j])\n",
    "        else:\n",
    "            lis.append(j)\n",
    "    document_after_stemming.append(lis)\n",
    "\n",
    "boolean_representation = []\n",
    "\n",
    "for i in range(0,len(document_after_stemming)):\n",
    "    lis = []\n",
    "    for j in vocab:\n",
    "        if j in  document_after_stemming[i]:\n",
    "            lis.append(1)\n",
    "        else:\n",
    "            lis.append(0)\n",
    "    boolean_representation.append(lis)\n",
    "\n",
    "boolean_representation\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
